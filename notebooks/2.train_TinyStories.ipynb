{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from utils import tokenize,tokenize_to_id,detokenize_to_text\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXTPATH = \"../../input/tinystories/TinyStoriesV2-GPT4-valid.txt\"\n",
    "TOKEN_ID_PATH = 'output/config/token_to_id_mapping.json'\n",
    "ID_TOKEN_PATH = 'output/config/id_to_token_mapping.json'\n",
    "SEQ_LEN = 32\n",
    "BATCH_SIZE = 1024\n",
    "EPOCHS = 1\n",
    "VOCAB_SIZE = 9772\n",
    "\n",
    "EMBED_DIM = 768\n",
    "TRANSFORMER_HEADS = 8\n",
    "TRANSFORMER_LAYERS = 2\n",
    "LR = 9e-5\n",
    "DEVICE=\"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19590883 2883876\n"
     ]
    }
   ],
   "source": [
    "with open(TEXTPATH) as f:\n",
    "    mainText = f.read()\n",
    "# mainText = \" \".join(mainText.split()[:10000])\n",
    "train_index = int(0.9*len(mainText.split(\" \")))\n",
    "train = \" \".join(mainText.split()[:train_index])\n",
    "valid = \" \".join(mainText.split()[train_index:])\n",
    "print(len(train),len(valid))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(TOKEN_ID_PATH) as json_file:\n",
    "    token_to_id_mapping = json.load(json_file)\n",
    "    token_to_id_mapping = json.loads(token_to_id_mapping)\n",
    "with open(ID_TOKEN_PATH) as json_file:\n",
    "    id_to_token_mapping = json.load(json_file)\n",
    "    id_to_token_mapping = json.loads(id_to_token_mapping)    \n",
    "TOKEN_TO_ID_MAPPING = token_to_id_mapping\n",
    "ID_TO_TOKEN_MAPPING = {int(t[0]):t[1] for t in id_to_token_mapping.items() }\n",
    "# token_to_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9771, 9771)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(list(token_to_id_mapping.values())),max([int(t) for t in list(id_to_token_mapping.keys())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TinyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,text):\n",
    "        self.text = text\n",
    "        \n",
    "        \n",
    "        self.tokens = tokenize(text)\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - SEQ_LEN\n",
    "    def __getitem__(self,idx):\n",
    "        x = self.tokens[idx:idx+SEQ_LEN]\n",
    "        y = self.tokens[idx+1:idx+SEQ_LEN+1]\n",
    "        x = tokenize_to_id(x,TOKEN_TO_ID_MAPPING)\n",
    "        y = tokenize_to_id(y,TOKEN_TO_ID_MAPPING)\n",
    "        x = torch.tensor(x,dtype=torch.long)\n",
    "        y = torch.tensor(y,dtype=torch.long)\n",
    "        return {'x':x,'y':y}\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://habr.com/en/companies/ods/articles/708672/\n",
    "class AttentionHead(nn.Module):\n",
    "    \"\"\"\n",
    "    One head of the self-attention layer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, head_size, num_embed, block_size):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        Initializes the AttentionHead module.\n",
    "\n",
    "        Args:\n",
    "            head_size (int): The size of each attention head.\n",
    "            num_embed (int): The dimension of the input embeddings.\n",
    "            block_size (int): The block size of the input sequence.\n",
    "        \"\"\"\n",
    "        self.key = nn.Linear(num_embed, head_size, bias=False)\n",
    "        self.query = nn.Linear(num_embed, head_size, bias=False)\n",
    "        self.value = nn.Linear(num_embed, head_size, bias=False)\n",
    "        # tril is a lower triangular matrix. it is not a parameter\n",
    "        # of the model, so we assign it to the module using register_buffer\n",
    "        self.register_buffer(\"tril\", torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the AttentionHead module.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): The input tensor of shape (B, T, C), where B is the batch size, T is the sequence length,\n",
    "                and C is the dimension of the input embeddings.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: The output tensor of shape (B, T, C).\n",
    "        \"\"\"\n",
    "        B, T, C = x.shape\n",
    "        k = self.key(x)\n",
    "        q = self.query(x)\n",
    "        # compute attention scores\n",
    "        # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
    "        # we need to transpose k to match q\n",
    "        wei = q @ k.transpose(-2, -1) * C**-0.5\n",
    "        # Tril matrix (lower triagular matrix) is used to mask \n",
    "        # future positions (setting them to -inf) so that the\n",
    "        # decoder \"learns\" to predict next words\n",
    "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float(\"-inf\"))  # (B,T,T)\n",
    "        wei = F.softmax(wei, dim=-1)  # (B,T,T)\n",
    "        # weighted aggregation of the values\n",
    "        v = self.value(x)\n",
    "        out = wei @ v  # (B,T,T) @ (B,T,C) ---> (B,T,C)\n",
    "        return out\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multiple Heads of self-attention in parallel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, head_size, num_embed, block_size):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadAttention module.\n",
    "\n",
    "        Args:\n",
    "            num_heads (int): The number of attention heads.\n",
    "            head_size (int): The size of each attention head.\n",
    "            num_embed (int): The dimension of the input embeddings.\n",
    "            block_size (int): The block size of the input sequence.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.heads = nn.ModuleList(\n",
    "            [\n",
    "                AttentionHead(\n",
    "                    head_size=head_size,\n",
    "                    num_embed=num_embed,\n",
    "                    block_size=block_size,\n",
    "                )\n",
    "                for _ in range(num_heads)\n",
    "            ]\n",
    "        )\n",
    "        self.proj = nn.Linear(num_embed, num_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # output of the self-attention\n",
    "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
    "        # apply the linear projection layer\n",
    "        out = self.proj(out)\n",
    "        return out    \n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    A simple linear layer followed by ReLu\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_embed):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            # in the Attention is All You Need paper\n",
    "            # authors are using the size of the ffwd layer 2048\n",
    "            # and the output of the model is 512\n",
    "            # so we apply the same factor of 4\n",
    "            nn.Linear(num_embed, 4 * num_embed),\n",
    "            nn.ReLU(),\n",
    "            # apply the linear projection layer\n",
    "            nn.Linear(4 * num_embed, num_embed),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Groups together MultiHeadAttention and FeedForward modules\n",
    "      to form a Transformer block.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_heads, block_size, num_embed):\n",
    "        super().__init__()\n",
    "        head_size = num_embed // num_heads\n",
    "        self.sa = MultiHeadAttention(\n",
    "            num_heads=num_heads,\n",
    "            head_size=head_size,\n",
    "            num_embed=num_embed,\n",
    "            block_size=block_size,\n",
    "        )\n",
    "        self.ffwd = FeedForward(num_embed=num_embed)\n",
    "        # add the layer normalization\n",
    "        self.ln1 = nn.LayerNorm(num_embed)\n",
    "        self.ln2 = nn.LayerNorm(num_embed)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # \"x +\" is the skip (or residual) connection\n",
    "        # it helps with optimization\n",
    "        # also we apply layer normalization before self-attention\n",
    "        # and feed-forward (a reshufle from original paper)\n",
    "        x = x + self.sa(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x    \n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__()\n",
    "        # a simple lookup table that stores embeddings of a fixed dictionary and size\n",
    "        # each token directly reads off the logits for the next token from a lookup table\n",
    "        # see more: https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html\n",
    "        self.vocab_size = kwargs.get(\"vocab_size\", 100)\n",
    "        self.num_embed = kwargs.get(\"num_embed\", 32)\n",
    "        self.block_size = kwargs.get(\"block_size\", 8)\n",
    "        self.num_heads = kwargs.get(\"num_heads\", 4)\n",
    "        self.num_layers = kwargs.get(\"num_layers\", 4)\n",
    "        # each token reads the logits for the next token from a lookup table\n",
    "        self.token_embedding_table = nn.Embedding(self.vocab_size, self.num_embed)\n",
    "        # each position from 0 to block_size-1 will get its embedding\n",
    "        self.position_embedding_table = nn.Embedding(self.block_size, self.num_embed)\n",
    "        self.blocks = nn.Sequential(\n",
    "            *[\n",
    "                TransformerBlock(\n",
    "                    num_heads=self.num_heads,\n",
    "                    block_size=self.block_size,\n",
    "                    num_embed=self.num_embed,\n",
    "                )\n",
    "                for _ in range(self.num_layers)\n",
    "            ]\n",
    "        )\n",
    "        # we add the layer norm before the Linear layer\n",
    "        self.ln_f = nn.LayerNorm(self.num_embed)\n",
    "        self.lm_head = nn.Linear(self.num_embed, self.vocab_size)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        B, T = idx.shape\n",
    "        # idx and targets are (B,T) tensor of integers\n",
    "        # the token_emb is (B, T, C), C = NUM_EMBED\n",
    "        token_emb = self.token_embedding_table(idx)\n",
    "        # (T, C)\n",
    "        posit_emb = self.position_embedding_table(torch.arange(T, device=DEVICE))\n",
    "\n",
    "        x = token_emb + posit_emb\n",
    "        # apply one head of self-attention\n",
    "        x = self.blocks(x)\n",
    "        # (B, T, vocab_size)\n",
    "        logits = self.lm_head(x)\n",
    "        # compute the loss\n",
    "        if targets != None:\n",
    "            # cross_entropy accepts inputs in a (batch_size, num_classes)\n",
    "            # so we need to reformat our logits dimensions to\n",
    "            # (batch_size * time, dim_vocabulary), time = block_size\n",
    "            B, T, C = logits.shape\n",
    "            logits = torch.reshape(logits, (B * T, C))\n",
    "            targets = torch.reshape(targets, (B * T,))\n",
    "            loss = F.cross_entropy(logits, targets)\n",
    "        else:\n",
    "            loss = None\n",
    "        return logits, loss\n",
    "\n",
    "    def generate(self, idx: torch.Tensor, max_new_tokens: int, block_size: int):\n",
    "        # idx is (B, T) array of indices in the current context\n",
    "        for _ in range(max_new_tokens):\n",
    "            # crop the context too the  last block_size tokens\n",
    "            # because tokens don't communicate between blocks\n",
    "            idx_crop = idx[:, -block_size:]\n",
    "            # get the predictions\n",
    "            logits, loss = self.forward(idx_crop)\n",
    "            # focus only on the last time step\n",
    "            logits = logits[:, -1, :]  # becomes (B, C)\n",
    "            # apply softmax to get probabilities\n",
    "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
    "            # sample from the distribution with probabilities probs\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
    "            # append sampled index to the running sequence\n",
    "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
    "        return idx    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TinyDataset(train)\n",
    "eval_data = TinyDataset(valid)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_data,\\\n",
    "                        batch_size=BATCH_SIZE,\\\n",
    "                        shuffle=True)\n",
    "eval_dataloader = torch.utils.data.DataLoader(eval_data,\\\n",
    "                    batch_size=BATCH_SIZE,\\\n",
    "                    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9317481, 1369773)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Transformer(vocab_size = VOCAB_SIZE,\n",
    "                    num_embed = EMBED_DIM,\n",
    "                    block_size = SEQ_LEN,\n",
    "                    num_heads = TRANSFORMER_HEADS,\n",
    "                    num_layers = TRANSFORMER_LAYERS)\n",
    "model.to(DEVICE)\n",
    "param_optimizer = model.parameters()\n",
    "optimizer = torch.optim.AdamW(param_optimizer, lr=LR)\n",
    "len(train_data),len(eval_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer,  dataloader, device, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        x = data['x'].to(device)\n",
    "        y = data['y'].to(device)\n",
    "        \n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        logits,loss = model.forward(x,y)\n",
    "        \n",
    "            \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "            \n",
    "                \n",
    "        running_loss += loss.item()\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    return epoch_loss  \n",
    "def valid_one_epoch(model, optimizer,  dataloader, device, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    with torch.no_grad():\n",
    "        for step, data in bar:\n",
    "            x = data['x'].to(device)\n",
    "            y = data['y'].to(device)\n",
    "            \n",
    "            batch_size = x.size(0)\n",
    "            \n",
    "            logits,loss = model.forward(x,y)\n",
    "            \n",
    "                    \n",
    "            running_loss += loss.item()\n",
    "            dataset_size += batch_size\n",
    "            \n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            \n",
    "            bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss,\n",
    "                            LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    return epoch_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9100/9100 [2:27:29<00:00,  1.03it/s, Epoch=1, LR=9e-5, Train_Loss=0.0014]   \n",
      "100%|██████████| 1338/1338 [13:06<00:00,  1.70it/s, Epoch=1, LR=9e-5, Train_Loss=0.00132]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  1\n",
      "{'Train Loss': 0.0013960368035055783}\n",
      "{'Valid Loss': 0.0013218676733231752}\n",
      "Validation Loss Improved (inf ---> 0.0013218676733231752)\n",
      "Model Saved\n",
      "###Evaluation###\n",
      "he   said   goodbye    \" of   course !  \"    he   showed   cindy   a   big   red   apple   and   said ,     \" hi\n",
      "\n",
      "\n",
      "once   upon   a   time ,    in   a   small   town ,    there   was   a   little   boy   named   tim .    tim   had\n",
      "\n",
      "\n",
      "he   was   the   best .    at   the   end   of   the   day ,    lily ' s   mom   and   dad   were   sad . \n",
      "\n",
      "\n",
      "Training complete in 2h 40m 39s\n",
      "Best Loss: 0.0013\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "best_epoch_loss = np.inf\n",
    "history = defaultdict(list)\n",
    "MODEL_DIR = 'output/modelwt'\n",
    "os.makedirs(MODEL_DIR,exist_ok=True)\n",
    "for epoch in range(1,EPOCHS+1):\n",
    "    gc.collect()\n",
    "    train_loss = train_one_epoch(model,optimizer,train_dataloader,DEVICE,epoch)\n",
    "\n",
    "    val_loss = valid_one_epoch(model,optimizer,eval_dataloader,DEVICE,epoch)\n",
    "\n",
    "    history[\"TrainLoss\"].append(train_loss)\n",
    "    history[\"ValLoss\"].append(val_loss)\n",
    "    print('EPOCH: ',epoch)\n",
    "    print({\"Train Loss\": train_loss})\n",
    "    print({\"Valid Loss\": val_loss})\n",
    "    \n",
    "    if val_loss < best_epoch_loss:\n",
    "        print(f\"Validation Loss Improved ({best_epoch_loss} ---> {val_loss})\")\n",
    "        \n",
    "        best_epoch_loss = val_loss\n",
    "        \n",
    "        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        PATH =\"model.pt\"\n",
    "        torch.save(model.state_dict(), os.path.join(MODEL_DIR,PATH))\n",
    "        # Save a model file from the current directory\n",
    "        print(\"Model Saved\")\n",
    "    print(\"###Evaluation###\")\n",
    "    evaluate_samples= [\"He said goodbye \",\"Once upon a time\",\"He was the best\"]\n",
    "    for sample in evaluate_samples:\n",
    "        tokens = tokenize(sample)\n",
    "        x = tokenize_to_id(tokens,TOKEN_TO_ID_MAPPING)\n",
    "        x = torch.tensor(x,dtype=torch.long).reshape(1,-1).to(DEVICE)\n",
    "        gen_seq = model.generate(idx=x, max_new_tokens=32, block_size=SEQ_LEN)\n",
    "        output = detokenize_to_text(list(gen_seq.cpu().detach().numpy()[0]), ID_TO_TOKEN_MAPPING)\n",
    "        print(output)\n",
    "        print(\"\\n\")\n",
    "end = time.time()\n",
    "time_elapsed = end - start\n",
    "print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "print(\"Best Loss: {:.4f}\".format(best_epoch_loss))\n",
    "PATH =\"lst_model.pt\"\n",
    "torch.save(model.state_dict(), os.path.join(MODEL_DIR,PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "he said goodbye - a reward. momo felt happy and excited. finally, the little bug found it stuck and he slipped and fell. ollie was a quick, but he still\n",
      "\n",
      "\n",
      "once upon a time there was a little girl who was playing in the mud. she was only three years old and she had a pet before. they wanted to go to another store\n",
      "\n",
      "\n",
      "he was the best grape had ever made! meadow was so excited. but when she arrived at the park, the boy found a big pile of soft stones. he saw a big\n",
      "\n",
      "\n",
      "in a land far far away. <|endoftext|> sally was playing and bob when he saw a new home. they wanted to play a trick on tv like wet. tom and sue decided to organize a\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "evaluate_samples= [\"He said goodbye \",\"Once upon a time\",\"He was the best\", \"in a land far far away\"]\n",
    "\n",
    "for sample in evaluate_samples:\n",
    "    tokens = tokenize(sample)\n",
    "    x = tokenize_to_id(tokens,TOKEN_TO_ID_MAPPING)\n",
    "    x = torch.tensor(x,dtype=torch.long).reshape(1,-1).to(DEVICE)\n",
    "    gen_seq = model.generate(idx=x, max_new_tokens=64, block_size=SEQ_LEN)\n",
    "    output = detokenize_to_text(list(gen_seq.cpu().detach().numpy()[0]), ID_TO_TOKEN_MAPPING)\n",
    "    print(output)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
